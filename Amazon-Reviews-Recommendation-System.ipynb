{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4FDZwD-u0AM"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z518ENaqu3Vy"
      },
      "source": [
        "The dataset consists of reviews for products from Amazon. We have the training and the testing data files which have been provided to us. The goal of this implementaation is to create a recommmender system using the training data and use the model to generate predicted ratings for each user-item pair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq58j3q3u6Bn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kprsw2X79Mtf"
      },
      "source": [
        "# **Loading libraries used in this code file**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2Tv0kChr_gP"
      },
      "source": [
        "## **Installing Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGKzaJSCsLb0"
      },
      "source": [
        "Normal installation of Numpy was not compatible with surprise library. Hence, I had to downgrade the version of numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTnAzclLBMxt"
      },
      "outputs": [],
      "source": [
        "!pip uninstall numpy\n",
        "!pip install numpy==1.24.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xfeMMTrsDM9"
      },
      "source": [
        "Scikit-surprise : Used to run SVD Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AJi1z5N8qvq"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-surprise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYTrwBpitHBS"
      },
      "source": [
        "Installing torch library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voutLhxQtGTc"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install lightgbm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAfMrW7-sXLs"
      },
      "source": [
        "## **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ScnY87C8zD6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from surprise import Dataset, Reader, SVD, accuracy, BaselineOnly, KNNBasic, NMF, SlopeOne, NormalPredictor\n",
        "from surprise.model_selection import cross_validate, train_test_split, GridSearchCV\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_squared_error, make_scorer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFh93hJcuIy1"
      },
      "source": [
        "# **Loading the Data Files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIwFrBsT9TSF"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7cAGQif90Xh"
      },
      "source": [
        "# **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9Hlpwbp9lkT"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMxfsN1K9s2F"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o1V25k59_kL"
      },
      "source": [
        "## **Count of Ratings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-x0Htmv9zUx"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "sns.histplot(train_df['rating'], bins=5, kde=False, color='steelblue', edgecolor='black')\n",
        "plt.title(\"Rating Distribution\")\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-zftnuc_UX5"
      },
      "source": [
        "## **Sparsity Calculation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsKzNpum_Nde"
      },
      "outputs": [],
      "source": [
        "n_users = train_df['user_id'].nunique()\n",
        "n_items = train_df['product_id'].nunique()\n",
        "n_ratings = len(train_df)\n",
        "\n",
        "sparsity = 1.0 - (n_ratings / (n_users * n_items))\n",
        "print(f\"Number of Users: {n_users}\")\n",
        "print(f\"Number of Products: {n_items}\")\n",
        "print(f\"Total Ratings: {n_ratings}\")\n",
        "print(f\"Matrix Sparsity: {sparsity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUsspYIM_ctF"
      },
      "source": [
        "## **Votes vs Helpful Votes Relation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD_Uv_Cd_TiY"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "sns.scatterplot(x='votes', y='helpful_votes', data=train_df, alpha=0.5)\n",
        "plt.title(\"Votes vs Helpful Votes\")\n",
        "plt.xlabel(\"Votes\")\n",
        "plt.ylabel(\"Helpful Votes\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation value\n",
        "correlation = train_df[['votes', 'helpful_votes']].corr().iloc[0, 1]\n",
        "print(f\"Correlation between Votes and Helpful Votes: {correlation:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTegp_ZT_j01"
      },
      "source": [
        "## **Average Rating across helpful votes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I31piRPM_gVT"
      },
      "outputs": [],
      "source": [
        "# Filter to non-zero vote entries to avoid noise\n",
        "filtered = train_df[train_df['votes'] > 0].copy()\n",
        "filtered['helpfulness_ratio'] = filtered['helpful_votes'] / filtered['votes']\n",
        "\n",
        "# Bin into ranges\n",
        "filtered['helpfulness_bin'] = pd.cut(filtered['helpfulness_ratio'], bins=[0, 0.25, 0.5, 0.75, 1.0])\n",
        "\n",
        "# Average rating by helpfulness bin\n",
        "avg_rating_by_helpfulness = filtered.groupby('helpfulness_bin')['rating'].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x='helpfulness_bin', y='rating', data=avg_rating_by_helpfulness, palette='Blues')\n",
        "plt.title(\"Average Rating by Helpfulness Ratio\")\n",
        "plt.xlabel(\"Helpfulness Ratio Bin\")\n",
        "plt.ylabel(\"Average Rating\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij_ifzuJ_0Bf"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDMeTi1wAD1w"
      },
      "outputs": [],
      "source": [
        "# Remove duplicate user-item ratings\n",
        "train_df.drop_duplicates(subset=['user_id', 'product_id'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Cz79CIvAKY5"
      },
      "outputs": [],
      "source": [
        "# Confirm no missing values\n",
        "print(train_df.isnull().sum())\n",
        "\n",
        "# Ensure ratings are numeric and in expected range\n",
        "print(train_df['rating'].describe())\n",
        "\n",
        "# Optional: drop out-of-range if needed\n",
        "train_df = train_df[train_df['rating'].between(1, 5)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aIHo91EA9qO"
      },
      "source": [
        "# **Data Modelling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdDYlXwM4Nw-"
      },
      "source": [
        "## **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlcZs7Vt4E5y"
      },
      "outputs": [],
      "source": [
        "# Define the rating scale for Surprise's Reader\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "# Convert data into a Surprise Dataset\n",
        "data = Dataset.load_from_df(train_df[['user_id', 'product_id', 'rating']], reader)\n",
        "\n",
        "# Split the dataset into training and testing sets using Surprise's train_test_split\n",
        "train, test = train_test_split(data, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH1ZkXu14Ik6"
      },
      "outputs": [],
      "source": [
        "# Converts the full dataset into a Surprise Trainset object\n",
        "trainset = data.build_full_trainset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fNtcBgn4KD_"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of users: {train_df['user_id'].nunique()}\")\n",
        "print(f\"Number of products: {train_df['product_id'].nunique()}\")\n",
        "print(f\"Number of ratings: {len(train_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8VCH1rC4RX1"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxR5EQMb39rG"
      },
      "source": [
        "## **NormalPredictor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpN0to-Y4A4K"
      },
      "outputs": [],
      "source": [
        "npstart = time.time()\n",
        "\n",
        "model = NormalPredictor()\n",
        "model.fit(trainset)\n",
        "\n",
        "npend = time.time()\n",
        "# Make predictions on test data\n",
        "predictions = model.test(test)\n",
        "\n",
        "# Compute RMSE\n",
        "rmse = accuracy.rmse(predictions)\n",
        "print(f\"RMSE on validation set: {rmse:.4f}\")\n",
        "\n",
        "print(f\"Training Time: {npend-npstart} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2i72B0g4Bcc"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMH0oCyk4WcH"
      },
      "source": [
        "## **Baseline Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGXbZIZm4fyb"
      },
      "source": [
        "### **BaselineOnly Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPd_BN-F4ceS"
      },
      "outputs": [],
      "source": [
        "bslstart = time.time()\n",
        "# Initializing BaselineOnly Model\n",
        "bsl_model = BaselineOnly()\n",
        "\n",
        "# Training\n",
        "bsl_model.fit(train)\n",
        "\n",
        "bslend = time.time()\n",
        "# Predict and evaluate\n",
        "bsl_preds = bsl_model.test(test)\n",
        "bsl_rmse = accuracy.rmse(bsl_preds)\n",
        "print(f\"BaselineOnly RMSE: {bsl_rmse:.4f}\")\n",
        "print(f\"Execution Time: {bslend-bslstart} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8Rvea6AE5Yb"
      },
      "source": [
        "### **BaselineOnly Hyperparameter Tuning using Grid search with Cross Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEAQbtzaE8-V"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'bsl_options': {\n",
        "        'method': ['sgd', 'als'],\n",
        "        'learning_rate': [0.001, 0.005],\n",
        "        'n_epochs': [10, 20],\n",
        "        'reg': [0.02, 0.05, 0.1]\n",
        "    }\n",
        "}\n",
        "\n",
        "gs_bsl = GridSearchCV(BaselineOnly, param_grid, measures=['rmse'], cv=3, n_jobs=-1, joblib_verbose=1)\n",
        "gs_bsl.fit(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUyWQ0JkE_xt"
      },
      "outputs": [],
      "source": [
        "print(f\"Best RMSE: {gs_bsl.best_score['rmse']:.4f}\")\n",
        "print(\"Best Params:\", gs_bsl.best_params['rmse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sqF-LLIFBW4"
      },
      "outputs": [],
      "source": [
        "tbslstart = time.time()\n",
        "# Retrive the best model\n",
        "tuned_bsl = gs_bsl.best_estimator['rmse']\n",
        "\n",
        "# Training\n",
        "tuned_bsl.fit(trainset)\n",
        "\n",
        "tbslend = time.time()\n",
        "\n",
        "# Predictions\n",
        "bsl_preds = [\n",
        "    tuned_bsl.predict(uid, iid).est\n",
        "    for uid, iid in zip(test_df['user_id'], test_df['product_id'])\n",
        "]\n",
        "\n",
        "# Save predictions\n",
        "test_df['rating'] = np.clip(bsl_preds, 1, 5)\n",
        "test_df[['ID', 'rating']].to_csv(\"baseline_tuned.csv\", index=False)\n",
        "print(f\"Execution Time: {tbslend-tbslstart} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKvaLVXlFt8k"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccczhPMOFZLx"
      },
      "source": [
        "## **Matrix Factorization Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkpdRA67CgFt"
      },
      "source": [
        "### **SVD (Singular Value Decomposition)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UF7J2ENC6KM"
      },
      "outputs": [],
      "source": [
        "# SVD Model Training\n",
        "svdstart = time.time()\n",
        "svd_model = SVD()\n",
        "svd_model.fit(train)\n",
        "svdend = time.time()\n",
        "\n",
        "predictions = svd_model.test(test)\n",
        "accuracy.rmse(predictions)\n",
        "print(f\"Execution Time: {svdend-svdstart} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPIBsE0eDGTq"
      },
      "outputs": [],
      "source": [
        "# Predict\n",
        "predicted_ratings = [\n",
        "    svd_model.predict(uid, iid).est\n",
        "    for uid, iid in zip(test_df['user_id'], test_df['product_id'])\n",
        "]\n",
        "\n",
        "# Create submission\n",
        "test_df['rating'] = predicted_ratings\n",
        "submission = test_df[['ID', 'rating']]\n",
        "submission.to_csv(\"Base_SVD.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORvX2IpGRG39"
      },
      "source": [
        "### **Fine tuning SVD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxZstxYsEoVN"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'n_factors': [50, 100, 150],\n",
        "    'lr_all': [0.005, 0.007, 0.01],\n",
        "    'reg_all': [0.02, 0.05, 0.08]\n",
        "}\n",
        "\n",
        "# Run grid search\n",
        "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3, n_jobs=-1, joblib_verbose=1)\n",
        "gs.fit(data)\n",
        "\n",
        "# Display best result\n",
        "print(\"Best RMSE:\", gs.best_score['rmse'])\n",
        "print(\"Best Parameters:\", gs.best_params['rmse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-9bE5wN0Dba"
      },
      "outputs": [],
      "source": [
        "# Tune the base SVD\n",
        "tuned_svd = SVD(n_factors=150, lr_all=0.01, reg_all=0.05)\n",
        "tsvd = time.time()\n",
        "tuned_svd.fit(trainset)\n",
        "tsvdend = time.time()\n",
        "\n",
        "predictions = tuned_svd.test(test)\n",
        "accuracy.rmse(predictions)\n",
        "print(f\"Execution Time: {tsvdend-tsvd} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbTipr2m1F-C"
      },
      "outputs": [],
      "source": [
        "test_preds = [\n",
        "    tuned_svd.predict(uid, iid).est\n",
        "    for uid, iid in zip(test_df['user_id'], test_df['product_id'])\n",
        "]\n",
        "\n",
        "# Clip to valid rating range\n",
        "test_df['rating'] = np.clip(test_preds, 1, 5)\n",
        "test_df['rating'] = np.floor(test_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYMxHEgO1J-M"
      },
      "outputs": [],
      "source": [
        "test_df[['ID', 'rating']].to_csv(\"tuned_svd.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVyU0dW3VAvV"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqR2njBrRdAl"
      },
      "source": [
        "## **Ensemble**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xNEK8jU3iKr"
      },
      "outputs": [],
      "source": [
        "# Define ensemble function\n",
        "def ensemble_predict(uid, iid, w_svd=0.6,w_bsl=0.4):\n",
        "    pred_svd = tuned_svd.predict(uid, iid).est  # 60% Weight to SVD\n",
        "    pred_bsl = bsl_model.predict(uid, iid).est  # 10% Weight to BaselineOnly\n",
        "    return w_svd * pred_svd + w_bsl * pred_bsl\n",
        "\n",
        "ensstart = time.time()\n",
        "# Generate predictions\n",
        "ensemble_preds = [\n",
        "    ensemble_predict(uid, iid)\n",
        "    for uid, iid in zip(test_df['user_id'], test_df['product_id'])\n",
        "]\n",
        "ensend = time.time()\n",
        "# Save to CSV\n",
        "test_df['rating'] = ensemble_preds\n",
        "test_df[['ID', 'rating']].to_csv(\"ensemble3.csv\", index=False)\n",
        "print(f\"Execution Time: {ensend-ensstart} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvyxt93jZAeX"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9BYP-sh6kHL"
      },
      "source": [
        "# **Appendix**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-bJelNquKK2"
      },
      "source": [
        "The below blocks of code was implemented to test various models. However, due to being computationally heavy, were not successfully implemented. The outputs for these blocks were cleared as they never executed completely and due to presence of the interrupt error, I cleared the output for these but kept the code blocks intact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9Kx1beE40yX"
      },
      "source": [
        "## **Memory Based Collaborative Filtering (KNN)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imSnnuegGsGZ"
      },
      "outputs": [],
      "source": [
        "# Define similarity measure\n",
        "sim_options = {\n",
        "    'name': 'cosine',\n",
        "    'user_based': False  # False = item-based\n",
        "}\n",
        "\n",
        "# Selecting KNNBasic Model\n",
        "knn_model = KNNBasic(sim_options=sim_options)\n",
        "\n",
        "# Training\n",
        "knn_model.fit(train)\n",
        "\n",
        "# Predict and evaluate\n",
        "knn_preds = knn_model.test(test)\n",
        "knn_rmse = accuracy.rmse(knn_preds)\n",
        "print(f\"KNNBasic (Item-Based) RMSE: {knn_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4I6MXhOA1_I"
      },
      "source": [
        "## **NMF (Non-Negative Matrix Factorization)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRz-2MJPA1oD"
      },
      "outputs": [],
      "source": [
        "# Initialize the NMF Model\n",
        "nmf_model = NMF()\n",
        "\n",
        "# Training\n",
        "nmf_model.fit(train)\n",
        "\n",
        "# Predict and evaluate\n",
        "nmf_preds = nmf_model.test(test)\n",
        "nmf_rmse = accuracy.rmse(nmf_preds)\n",
        "print(f\"NMF RMSE: {nmf_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53O3MT6zA9QE"
      },
      "source": [
        "### **Hyperparameter Tuning (GridSearch)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fblGvx63A_Yj"
      },
      "outputs": [],
      "source": [
        "# Defining the parameters grid\n",
        "param_grid = {\n",
        "    'n_factors': [50, 100, 150],\n",
        "    'reg_pu': [0.02, 0.05],\n",
        "    'reg_qi': [0.02, 0.05],\n",
        "    'biased': [True, False]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G02_arHTBCx1"
      },
      "outputs": [],
      "source": [
        "# Set up GridSearchCV for NMF\n",
        "gs_nmf = GridSearchCV(\n",
        "    NMF, param_grid,\n",
        "    measures=['rmse'], cv=3,\n",
        "    n_jobs=-1,        # Use all cores\n",
        "    joblib_verbose=2  # More detailed logs\n",
        ")\n",
        "gs_nmf.fit(data)\n",
        "print(\"Best RMSE:\", gs_nmf.best_score['rmse'])\n",
        "print(\"Best Parameters:\", gs_nmf.best_params['rmse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJbDeFBgBEkr"
      },
      "outputs": [],
      "source": [
        "# Best score and params\n",
        "print(\"Best NMF RMSE:\", gs_nmf.best_score['rmse'])\n",
        "print(\"Best NMF params:\", gs_nmf.best_params['rmse'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peOKJdkjgvU9"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fT9-oFkR5Tv"
      },
      "source": [
        "## **NCF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKJNx_wf7cR_"
      },
      "outputs": [],
      "source": [
        "# Encode user and item IDs\n",
        "user2idx = {u: i for i, u in enumerate(train_df['user_id'].unique())}\n",
        "item2idx = {i: j for j, i in enumerate(train_df['product_id'].unique())}\n",
        "train_df['user_idx'] = train_df['user_id'].map(user2idx)\n",
        "train_df['item_idx'] = train_df['product_id'].map(item2idx)\n",
        "\n",
        "# PyTorch Data creation Pipeline\n",
        "class NCFDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.users = torch.tensor(df['user_idx'].values, dtype=torch.long)\n",
        "        self.items = torch.tensor(df['item_idx'].values, dtype=torch.long)\n",
        "        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ratings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.items[idx], self.ratings[idx]\n",
        "\n",
        "# Splitting Train and Validation Data\n",
        "train_set, val_set = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train set\n",
        "train_dataset = NCFDataset(train_set)\n",
        "\n",
        "# Validation Set\n",
        "val_dataset = NCFDataset(val_set)\n",
        "\n",
        "# Wrapping in dataloaders for batch training\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVsrQiG1biZ-"
      },
      "source": [
        "### **Model ready for training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f0rMqYy8lij"
      },
      "outputs": [],
      "source": [
        "class NCF(nn.Module):\n",
        "  # Initializing\n",
        "    def __init__(self, n_users, n_items, emb_size=50):\n",
        "        super(NCF, self).__init__()\n",
        "\n",
        "        # Embedding Layers\n",
        "        self.user_emb = nn.Embedding(n_users, emb_size)\n",
        "        self.item_emb = nn.Embedding(n_items, emb_size)\n",
        "\n",
        "        # Fully Connected Neural Network\n",
        "        self.fc1 = nn.Linear(emb_size * 2, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.out = nn.Linear(64, 1)\n",
        "\n",
        "    # Forward Pass\n",
        "    def forward(self, user, item):\n",
        "        u = self.user_emb(user)\n",
        "        i = self.item_emb(item)\n",
        "        x = torch.cat([u, i], dim=-1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.out(x).squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E7pNM3O8n23"
      },
      "outputs": [],
      "source": [
        "# Setting GPU or else use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Model Initialization\n",
        "model = NCF(len(user2idx), len(item2idx)).to(device)\n",
        "\n",
        "# Loss Function and Optimizer(Adam)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):   # Running the loop for 10 epochs\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for users, items, ratings in train_loader:\n",
        "        users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
        "        preds = model(users, items)\n",
        "        loss = criterion(preds, ratings)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train RMSE: {total_loss/len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob6nXf29GF2a"
      },
      "outputs": [],
      "source": [
        "# Initializing evaluation of model\n",
        "model.eval()\n",
        "\n",
        "# List to store predicted and target values\n",
        "val_preds = []\n",
        "val_targets = []\n",
        "\n",
        "# Looping through validation set\n",
        "with torch.no_grad():\n",
        "    for users, items, ratings in val_loader:\n",
        "        users, items = users.to(device), items.to(device)\n",
        "        preds = model(users, items).cpu().numpy()\n",
        "        val_preds.extend(preds)\n",
        "        val_targets.extend(ratings.numpy())\n",
        "\n",
        "# Evaluation\n",
        "val_rmse = np.sqrt(np.mean((np.array(val_preds) - np.array(val_targets)) ** 2))\n",
        "print(f\"Validation RMSE: {val_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKUWFuiMGlkn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbZhRdKoeYVw"
      },
      "source": [
        "### **NCF with Dropout Regularization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MhQ91yyGmIQ"
      },
      "outputs": [],
      "source": [
        "class NCF(nn.Module):\n",
        "    def __init__(self, n_users, n_items, emb_size=16):\n",
        "        super(NCF, self).__init__()\n",
        "\n",
        "        # Embedding layers\n",
        "        self.user_emb = nn.Embedding(n_users, emb_size)\n",
        "        self.item_emb = nn.Embedding(n_items, emb_size)\n",
        "\n",
        "        # Fully connected layer and dropout\n",
        "        self.fc1 = nn.Linear(emb_size * 2, 64)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.out = nn.Linear(64, 1)\n",
        "\n",
        "    # Forward Pass\n",
        "    def forward(self, user, item):\n",
        "        u = self.user_emb(user)\n",
        "        i = self.item_emb(item)\n",
        "        x = torch.cat([u, i], dim=-1)\n",
        "        x = torch.relu(self.dropout(self.fc1(x)))\n",
        "\n",
        "        # Output Layer\n",
        "        return self.out(x).squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbPOX7vxGoh3"
      },
      "outputs": [],
      "source": [
        "model = NCF(len(user2idx), len(item2idx)).to(device)\n",
        "\n",
        "# Loss criteria and Optimizer (Adam)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-5)\n",
        "\n",
        "#\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for users, items, ratings in train_loader:\n",
        "        users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
        "\n",
        "        # Forward Pass -> Compute Loss -> Backpropogation\n",
        "        preds = model(users, items)\n",
        "        loss = criterion(preds, ratings)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Train RMSE: {total_loss / len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9ZfKcR-HwpQ"
      },
      "outputs": [],
      "source": [
        "# Setting model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Creating list for predictions and targets\n",
        "val_preds, val_targets = [], []\n",
        "with torch.no_grad():\n",
        "    # Looping without gradient decent\n",
        "    for users, items, ratings in val_loader:\n",
        "        users, items = users.to(device), items.to(device)\n",
        "        preds = model(users, items).cpu().numpy()\n",
        "        val_preds.extend(preds)\n",
        "        val_targets.extend(ratings.numpy())\n",
        "\n",
        "# Evaluation\n",
        "val_rmse = np.sqrt(np.mean((np.array(val_preds) - np.array(val_targets)) ** 2))\n",
        "print(f\"Validation RMSE: {val_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srm1DjRVH9Kb"
      },
      "outputs": [],
      "source": [
        "# Encode using train mappings\n",
        "test_df['user_idx'] = test_df['user_id'].map(user2idx)\n",
        "test_df['item_idx'] = test_df['product_id'].map(item2idx)\n",
        "\n",
        "# Handle new/unseen users/items\n",
        "test_df['user_idx'] = test_df['user_idx'].fillna(0).astype(int)\n",
        "test_df['item_idx'] = test_df['item_idx'].fillna(0).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBtgRsALIBB_"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "# Testing Tensors\n",
        "test_users = torch.tensor(test_df['user_idx'].values, dtype=torch.long).to(device)\n",
        "test_items = torch.tensor(test_df['item_idx'].values, dtype=torch.long).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Predictions without gradients\n",
        "    test_preds = model(test_users, test_items).cpu().numpy()\n",
        "    test_preds = np.clip(test_preds, 1, 5)\n",
        "\n",
        "test_df['rating'] = test_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI_TLABOIDHD"
      },
      "outputs": [],
      "source": [
        "# Creating prediction file\n",
        "test_df[['ID', 'rating']].to_csv(\"ncf_tuned.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6s0yQtKgTry"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
